{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Árboles de Decisión\n",
    "\n",
    "Veremos árboles de decisión y los conceptos subyacentes asociados.\n",
    "\n",
    "Haremos ejemplos de juguete y con datos generados artificialmente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entropía y Ganancia de Información"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entropía\n",
    "\n",
    "Definamos entropía para una distribución probabilista:\n",
    "\n",
    "$$H(Y) = - \\sum_{i=1}^k P(Y = y_i) log_2 P(Y = y_i)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(probs):\n",
    "    return - np.sum(probs * np.log2(probs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos posibles entropías para el problema de tirar una moneda adulterada:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entropy(np.array([0.5, 0.5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entropy(np.array([0.01, 0.99]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.linspace(0, 1)[1:-1]\n",
    "plt.plot(X, [entropy([x, 1-x]) for x in X])\n",
    "plt.xlabel('P(Y=y_1)')\n",
    "plt.ylabel('entropy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La entropía se puede interpretar como la cantidad de bits necesarias para codificar una predicción.\n",
    "En el caso de dos monedas, tenemos cuatro resultados posibles. Ejemplos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entropy(np.array([0.25, 0.25, 0.25, 0.25]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entropy(np.array([0.48, 0.48, 0.01, 0.01]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entropía de un Dataset\n",
    "\n",
    "Un dataset define una distribución empírica. La entropía del dataset es entones la entropía de la distribución asociada. Definamos el cálculo de la distribución, y luego redefinamos entropía:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def probs(y):\n",
    "    _, counts = np.unique(y, return_counts=True)\n",
    "    probs = counts / counts.sum()\n",
    "    return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = np.array([1, 1, 1, 1, 1, -1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs(y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(y):\n",
    "    p = probs(y)\n",
    "    return - np.sum(p * np.log2(p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entropy(y_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entropía Condicional\n",
    "\n",
    "Definamos entropía condicional:\n",
    "\n",
    "$$H(Y|X) = - \\sum_{j=1}^v P(X = x_j) \\sum_{i=1}^k P(Y = y_i | X = x_j) log_2 P(Y = y_i | X = x_j)$$\n",
    "\n",
    "Equivalentemente, \n",
    "\n",
    "$$H(Y|X) = \\sum_{j=1}^v P(X = x_j) H(Y|X = x_j)$$\n",
    "\n",
    "Tomaremos $X$ binaria ($v=2$), por lo que la entropía condicional tendrá sólo dos términos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cond_entropy(y1, y2):\n",
    "    size = y1.shape[0] + y2.shape[0]\n",
    "    return y1.shape[0] / size * entropy(y1) + y2.shape[0] / size * entropy(y2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cond_entropy(np.array([1,1,1,1]), np.array([1,-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ganancia de Información\n",
    "\n",
    "La ganancia de información será simplemente la diferencia entre entropía y entropía condicional:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def information_gain(y1, y2):\n",
    "    y = np.concatenate((y1,y2))\n",
    "    return entropy(y) - cond_entropy(y1,y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "information_gain(np.array([1,1,1,1]), np.array([1,-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "information_gain(np.array([1,1,1]), np.array([1,1,-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datos Sintéticos No Linealmente Separables\n",
    "\n",
    "Haremos algunos experimentos con datos generados sintéticamente. Estos datos serán no linealmente separables.\n",
    "\n",
    "Ejemplos típicos de datos no linealmente separables son los de tipo \"OR\", \"AND\" y \"XOR\". Usaremos datos de tipo \"OR\" para este ejemplo.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "X = np.random.randn(size, 2)\n",
    "y_true = np.logical_or(X[:, 0] > 0, X[:, 1] > 0)    # datos \"OR\"\n",
    "#y_true = np.logical_xor(X[:, 0] > 0, X[:, 1] > 0)  # datos \"XOR\"\n",
    "#y_true = np.logical_and(X[:, 0] > 0, X[:, 1] > 0)  # datos \"AND\"\n",
    "y_true = y_true.astype(int)\n",
    "y_true[y_true == 0] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X[y_true==1, 0], X[y_true==1, 1], color=\"dodgerblue\", edgecolors='k', label=\"1\")\n",
    "plt.scatter(X[y_true==-1, 0], X[y_true==-1, 1], color=\"tomato\", edgecolors='k', label=\"-1\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### División en Entrenamiento y Evaluación\n",
    "\n",
    "Separemos la mitad para entrenamiento y la otra para evaluación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = 100\n",
    "test_size = size - train_size\n",
    "X_train, X_test = X[:train_size], X[train_size:]\n",
    "y_train, y_test = y_true[:train_size], y_true[train_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clasificación Lineal\n",
    "\n",
    "Veamos qué tan mal anda un clasificador lineal sobre estos datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import plot_decision_boundary\n",
    "\n",
    "plot_decision_boundary(lambda x: model.predict(x), X, y_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculemos la calidad de la predicción en entrenamiento y evaluación:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred = model.predict(X_train)\n",
    "y_test_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "train_acc = accuracy_score(y_train, y_train_pred)\n",
    "test_acc = accuracy_score(y_test, y_test_pred)\n",
    "print(f'Train accuracy: {train_acc:0.2}')\n",
    "print(f'Test accuracy: {test_acc:0.2}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nota al Margen: Induciendo Separabilidad Lineal\n",
    "\n",
    "Muchas veces se pueden convertir datos no linealmente separables en datos separables (o casi) mediante la introducción de nuevos atributos que combinan los atributos existentes.\n",
    "Un ejemplo de estos son los atributos polinomiales.\n",
    "\n",
    "Aquí lo haremos con datos \"OR\", pero la diferencia es mucho más notable con datos de tipo \"XOR\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "pre = PolynomialFeatures(degree=2, interaction_only=True, include_bias=False)\n",
    "X_train2 = pre.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grafiquemos "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X_train2[y_train==1, 1], X_train2[y_train==1, 2], color=\"dodgerblue\", edgecolors='k', label=\"1\")\n",
    "plt.scatter(X_train2[y_train==-1, 1], X_train2[y_train==-1, 2], color=\"tomato\", edgecolors='k', label=\"-1\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "model = make_pipeline(\n",
    "    PolynomialFeatures(degree=2, interaction_only=True, include_bias=False),\n",
    "    LogisticRegression()\n",
    ")\n",
    "model.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred = model.predict(X_train)\n",
    "y_test_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "train_acc = accuracy_score(y_train, y_train_pred)\n",
    "test_acc = accuracy_score(y_test, y_test_pred)\n",
    "print(f'Train accuracy: {train_acc:0.2}')\n",
    "print(f'Test accuracy: {test_acc:0.2}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entropía y Valores Reales\n",
    "\n",
    "Calculemos la entropía inicial, y veamos cómo condicionar la entropía sobre variales reales (i.e. no categóricas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entropy(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para hacer una división sobre una variable real usaremos un valor \"threshold\" (umbral):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split(X, y, i, threshold):\n",
    "    y1 = y[X[:, i] > threshold]\n",
    "    y2 = y[X[:, i] <= threshold]\n",
    "    return y1, y2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y1, y2 = split(X_train, y_train, 0, 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y1,y2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entropy(y1), entropy(y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cond_entropy(y1, y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "information_gain(y1,y2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Buscando la Mejor División\n",
    "\n",
    "Ilustraremos un paso en la construcción del árbol de decisión.\n",
    "\n",
    "Probemos muchos threshold para ambas variables y seleccionemos la mejor división.\n",
    "\n",
    "En este caso buscaremos en una grilla uniforme de valores, pero existen técnicas mejores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linspace(-2.5, 2.5, 11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_ig = 0\n",
    "\n",
    "for i in [0, 1]:\n",
    "    for threshold in np.linspace(-2.5, 2.5, 11):\n",
    "        y1, y2 = split(X_train, y_train, i, threshold)\n",
    "        ig = information_gain(y1, y2)\n",
    "        print(f'i={i}\\tthreshold={threshold:+00.2f}\\tig={ig:.2f}')\n",
    "        \n",
    "        if ig >= best_ig:\n",
    "            best_ig = ig\n",
    "            best_feature = i\n",
    "            best_threshold = threshold\n",
    "\n",
    "print('Mejor división:')\n",
    "print(f'feature={best_feature}, threshold={best_threshold}, ig={best_ig:00.2f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dividamos los datos de acuerdo a esta frontera:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y1, y2 = split(X_train, y_train, best_feature, best_threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con esta división, la entropía baja considerablemente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entropy(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cond_entropy(y1, y2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Árbol de Decisión con Scikit-learn\n",
    "\n",
    "Aprendamos un árbol de decisión usando scikit-learn. Para ello usaremos la clase [DecisionTreeClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "clf = DecisionTreeClassifier(max_depth=3, criterion='entropy', random_state=0)\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora predecimos y evaluamos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred = clf.predict(X_train)\n",
    "y_test_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "train_acc = accuracy_score(y_train, y_train_pred)\n",
    "test_acc = accuracy_score(y_test, y_test_pred)\n",
    "print(f'Train accuracy: {train_acc:0.2}')\n",
    "print(f'Test accuracy: {test_acc:0.2}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dibujamos la frontera de decisión:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import plot_decision_boundary\n",
    "\n",
    "plot_decision_boundary(lambda x: clf.predict(x), X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "También podemos inspeccionar el árbol:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import plot_tree\n",
    "\n",
    "plot_tree(clf);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicios\n",
    "\n",
    "1. Probar todos los experimentos con el dataset de tipo \"XOR\". ¿Qué sucede al decidir la división en el primer nivel del árbol?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Referencias\n",
    "\n",
    "Scikit-learn:\n",
    "\n",
    "- [accuracy_score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html)\n",
    "- [User Guide: Decision Trees](https://scikit-learn.org/stable/modules/tree.html)\n",
    "- [DecisionTreeClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
